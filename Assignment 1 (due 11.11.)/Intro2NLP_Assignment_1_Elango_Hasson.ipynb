{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a325897",
   "metadata": {
    "id": "5a325897"
   },
   "source": [
    "# Introduction to Natural Language Processing: Assignment 1\n",
    "\n",
    "In this assignment we'll practice word operations and text classifications.\n",
    "\n",
    "- Please comment your code\n",
    "- You can use built-in Python packages, scikit-learn and Pandas.\n",
    "- Submissions are due **on Tuesdays at 23:59** and should be submitted **ONLY** on eCampus: **Assignmnets >> Student Submissions >> Assignment 1 (Deadline: 05.11.2024, at 23:59)**\n",
    "- Name the file aproppriately \"Assignment_1_\\<Your_Name\\>.ipynb\".\n",
    "- Please submit **ONLY** the Jupyter Notebook file.\n",
    "- Please use relative path; Your code should work on my computer if the Jupyter Notebook and the file are both in the same directory.\n",
    "\n",
    "Example: file_name = lemmatization-en.txt >> **DON'T use:** /Users/ComputerName/Username/Documents/.../lemmatization-en.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd8bf33",
   "metadata": {
    "id": "0cd8bf33"
   },
   "source": [
    "### Task 1.1 (2 points)\n",
    "\n",
    "Write a function `extract_words_tokens(any_string)` that takes a string as input and returns two numbers:\n",
    "1. num_words: The number of words in string\n",
    "2. num_tokens: The number of tokens in string (Please use the character-based tokenization.)\n",
    "\n",
    "**Hint:** The string can be a single word or a sentence and\n",
    " can contain some special charecters, such as: \"!\", \",\", \":\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14f3124",
   "metadata": {
    "id": "f14f3124"
   },
   "outputs": [],
   "source": [
    "def extract_words_tokens(any_string):\n",
    "    # splits the string by the spaces it has and counts the resulting amount of words\n",
    "    num_words = len(any_string.split())\n",
    "    # deletes the spaces within the string and then counts the amount of characters\n",
    "    num_tokens = len(any_string.replace(' ', ''))\n",
    "    return(print(any_string, \":\", \"num_words:\", num_words, \"and\", \"num_tokens:\", num_tokens, \"respectively\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa8c8d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, how are you? My name is computer, nice to meet you! : num_words: 12 and num_tokens: 47 respectively\n"
     ]
    }
   ],
   "source": [
    "extract_words_tokens('Hello, how are you? My name is computer, nice to meet you!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b05add",
   "metadata": {
    "id": "a4b05add"
   },
   "source": [
    "### Task 1.2 (4 points)\n",
    "\n",
    "Write a function `lemmatize(any_string, file_name)` that takes as input any string and a file-name: `lemmatization-en.txt` (please download the file [here](https://github.com/michmech/lemmatization-lists/blob/master/lemmatization-en.txt). It's a tab separated corpus) and returns a dictionary with all words as keys and the lemma of the words as values.\n",
    "\n",
    "**Hint:** To tokenize the string, please use the whitespace as the seperator. The string doesn't contain any special characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12f48ff",
   "metadata": {
    "id": "a12f48ff"
   },
   "outputs": [],
   "source": [
    "def lemmatize(any_string, file_name):\n",
    "    lemmas = {}\n",
    "    with open(file_name, 'r') as f:\n",
    "        for line in f:\n",
    "            \n",
    "    dictionary_of_lemmatized_words = {}\n",
    "    return(print(dictionary_of_lemmatized_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ebaa2255",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mlemmatization-en.txt\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fh:\n\u001b[32m      2\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m fh:\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m         a,b = line.split()\n\u001b[32m      4\u001b[39m         \u001b[38;5;66;03m#print(a)\u001b[39;00m\n\u001b[32m      5\u001b[39m         \u001b[38;5;66;03m#print(b)\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "with open('lemmatization-en.txt', 'r') as fh:\n",
    "    for line in fh:\n",
    "        a,b = line.split()\n",
    "        #print(a)\n",
    "        #print(b)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4e33ecf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ï»¿1'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7b04cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatize('hello how are you', 'lemmatization-en.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5142598",
   "metadata": {},
   "source": [
    "### Task 2 (1 point)\n",
    "\n",
    "Create a DataFrame using the `polarity.txt` file and give name to the columns appropriately. (e.g., \"Text\", \"Label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e400715a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>every now and then a movie comes along from a ...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mtv films' _election , a high school comedy st...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>did anybody know this film existed a week befo...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the plot is deceptively simple .</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>george washington carver high school is having...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text Label\n",
       "0  every now and then a movie comes along from a ...   pos\n",
       "1  mtv films' _election , a high school comedy st...   pos\n",
       "2  did anybody know this film existed a week befo...   pos\n",
       "3                  the plot is deceptively simple .    pos\n",
       "4  george washington carver high school is having...   pos"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "dixi = {'Text': [], 'Label': []}\n",
    "with open('polarity.txt', 'r') as fh:\n",
    "    for line in fh:\n",
    "        a,b = re.split(r'\\t+', line.rstrip('\\t'))\n",
    "        b = b.replace('\\n', '')\n",
    "        dixi['Text'].append(a)\n",
    "        dixi['Label'].append(b)\n",
    "\n",
    "df = pd.DataFrame(dixi)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b5eaf4",
   "metadata": {},
   "source": [
    "### Task 2.1 (2 point)\n",
    "\n",
    "Create a new column for the DataFrame that contains labels converted to numerical values instead of strings using the function: `apply()` and drop the original column afterwards.\n",
    "\n",
    "Hint: The numarical values can be any meaningful values, e.g., pos >> 1 and neg >> 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8309aebd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "      <th>Num_Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>it also wrapped production two years ago and h...</td>\n",
       "      <td>neg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>whatever . . . skip</td>\n",
       "      <td>neg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>it !</td>\n",
       "      <td>neg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>where's joblo coming from ?</td>\n",
       "      <td>neg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>a nightmare of elm street 3 ( 7/10 ) - blair w...</td>\n",
       "      <td>pos</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Text Label  Num_Label\n",
       "69  it also wrapped production two years ago and h...   neg          0\n",
       "70                               whatever . . . skip    neg          0\n",
       "71                                              it !    neg          0\n",
       "72                       where's joblo coming from ?    neg          0\n",
       "73  a nightmare of elm street 3 ( 7/10 ) - blair w...   pos          1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def label2num(lab):\n",
    "    if lab == 'pos':\n",
    "        return 1\n",
    "    elif lab == 'neg':\n",
    "        return 0\n",
    "    else:\n",
    "        return None\n",
    "df['Num_Label'] = df['Label'].apply(label2num)\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b7e88b",
   "metadata": {},
   "source": [
    "### Task 3 (7 points)\n",
    "\n",
    "Write a function `create_count_and_probability` that takes a file (`corpus.txt`) as input and returns a csv file as output containing three columns:\n",
    "1. Text\n",
    "2. Count_Vector\n",
    "3. Probability\n",
    "\n",
    "Example:\n",
    "\n",
    "For the line: `This document is the second document.`\n",
    "\n",
    "The row in the csv file should contain:\n",
    "`This document is the second document.`   `[0,2,0,1,0,1,1,0,1]`   `[1/6, 2/6, 1/6, 1/6, 1/6, 2/6]`\n",
    "\n",
    "**Note**:\n",
    "\n",
    "1. You should define your own function and not use e.g., CountVectorizer() which gives you the `count vector`, directly.\n",
    "\n",
    "2. You can either use the whitespace in `split` as the seperator or use the `Regular Expression (re)` to extract the words, as follows:\n",
    "\n",
    "```\n",
    "import re\n",
    "TEXT = \"Hey, - How are you doing today!?\"\n",
    "words_list = re.findall(r\"[\\w']+\", TEXT)\n",
    "print(words_list)\n",
    "```\n",
    "\n",
    "3. To count the words, you can use e.g., the library: `collections`, more specifically `Counter`.\n",
    "\n",
    "4. Please don't upload the output file. Your function should generate the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26f9f9be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Count_Vector</th>\n",
       "      <th>Probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This is the first document.</td>\n",
       "      <td>[0, 1, 1, 1, 0, 0, 1, 0, 1]</td>\n",
       "      <td>[1/5, 1/5, 1/5, 1/5, 1/5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This document is the second document.</td>\n",
       "      <td>[0, 2, 0, 1, 0, 1, 1, 0, 1]</td>\n",
       "      <td>[1/6, 2/6, 1/6, 1/6, 1/6]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>And this is the third one.</td>\n",
       "      <td>[1, 0, 0, 1, 1, 0, 1, 1, 1]</td>\n",
       "      <td>[1/6, 1/6, 1/6, 1/6, 1/6, 1/6]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Is this the first document?</td>\n",
       "      <td>[0, 1, 1, 1, 0, 0, 1, 0, 1]</td>\n",
       "      <td>[1/5, 1/5, 1/5, 1/5, 1/5]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Text                 Count_Vector  \\\n",
       "0            This is the first document.  [0, 1, 1, 1, 0, 0, 1, 0, 1]   \n",
       "1  This document is the second document.  [0, 2, 0, 1, 0, 1, 1, 0, 1]   \n",
       "2             And this is the third one.  [1, 0, 0, 1, 1, 0, 1, 1, 1]   \n",
       "3            Is this the first document?  [0, 1, 1, 1, 0, 0, 1, 0, 1]   \n",
       "\n",
       "                      Probability  \n",
       "0       [1/5, 1/5, 1/5, 1/5, 1/5]  \n",
       "1       [1/6, 2/6, 1/6, 1/6, 1/6]  \n",
       "2  [1/6, 1/6, 1/6, 1/6, 1/6, 1/6]  \n",
       "3       [1/5, 1/5, 1/5, 1/5, 1/5]  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "def create_count_and_probability(file_name):\n",
    "    csv_file = {'Text': [], 'Count_Vector': [], 'Probability': []}\n",
    "    all_words = []\n",
    "    with open(file_name, 'r') as fh:\n",
    "        for line in fh:\n",
    "            # save the text lines from the file\n",
    "            text = line.replace('\\n', '')\n",
    "            csv_file['Text'].append(text)\n",
    "            # save all words in lowercase letters\n",
    "            words_list = re.findall(r\"[\\w']+\", text.lower())\n",
    "            for word in words_list:\n",
    "                all_words.append(word)\n",
    "    # get all unique words in order\n",
    "    unique_words = np.unique(all_words).tolist()\n",
    "    for line_text in csv_file['Text']:\n",
    "        # generate a list with zeros for each of the words in the file\n",
    "        count_vector = np.zeros_like(unique_words, dtype=int)\n",
    "        # split the line in lowercase words and count them\n",
    "        words_list = re.findall(r\"[\\w']+\", line_text.lower())\n",
    "        line_count = Counter(words_list)\n",
    "        # generate a list for the probabilities\n",
    "        probability = np.zeros_like(list(line_count.keys()))\n",
    "        line_len = str(len(words_list))\n",
    "        # iterate through the words in the line and the counts to generate the count vector and adds the probability to find it in another vector\n",
    "        for word, count in line_count.items():\n",
    "            count_vector[unique_words.index(word)] += count\n",
    "            probability[list(line_count.keys()).index(word)] = str(count) + '/' + line_len\n",
    "        csv_file['Count_Vector'].append(count_vector.tolist())\n",
    "        csv_file['Probability'].append(probability.tolist())\n",
    "    csv_file = pd.DataFrame(csv_file)\n",
    "    csv_file.to_csv(file_name[:-4] + 'counts_and_probabilities.csv', index=False)\n",
    "    return(csv_file)\n",
    "\n",
    "create_count_and_probability('corpus.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e701a2ce",
   "metadata": {},
   "source": [
    "### Task 4 (8 points)\n",
    "\n",
    "The goal of this task is to train and test classifiers provided in scikit-learn, using two datasets `rural.txt` and `science.txt`.\n",
    "\n",
    "a) Each file (rural and science) contains sentence-wise documents. You should create a dataframe containing two columns: \"Document\" and \" Class\", as shown below. This dataframe will be used later as input for the vectorizer.\n",
    "\n",
    "|Document                             |Class |\n",
    "| ------------------------------------|----- |\n",
    "|PM denies knowledge of AWB kickbacks | rural |\n",
    "|The crocodile ancestor fossil, found...| science |\n",
    "\n",
    "\n",
    "b) Remove stop words from the data and create two separate plots showing word frequency for documents in each label.\n",
    "\n",
    "c) Split the data into train (70%) and test (30%) sets and use the following vectorization techniquess to train the two classifiers provided by scikit-learn:\n",
    "\n",
    "- one-hot-encoding\n",
    "- count vectorization\n",
    "\n",
    "Classifiers:\n",
    "- naive_bayes.GaussianNB()\n",
    "- LogisticRegression()\n",
    "\n",
    "**Hints:**\n",
    "1. The Gaussian NB Classifier takes a dense matrix as input and the output of the vectorizer is a sparse matrix. Use my_matrix.toarray() for this conversion.\n",
    "2. You can play around with various parameters in both the count-vectorizer and the classifier to get a better performance in terms of the accuracy. (In the exercise, we will discuss the accuracy of your model.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19367cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here comes your code"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv (3.12.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
